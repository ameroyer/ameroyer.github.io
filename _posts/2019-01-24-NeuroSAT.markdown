---
layout: post
title:  "Learning a SAT solver with Single-Bit supervision"
date:   2019-01-24 18:00:00 +0200
tags: deep learning
categories: readingnotes
thumb: /images/thumbs/neurosat.png
---


This post contains my reading notes about the  <span class="keyword">NeuroSAT</span> architecture for learning the satisfiability of SAT formulas, as described in the paper <a href="https://arxiv.org/abs/1802.03685" target="_blank"><i>"Learning a SAT Solver from Single-Bit Supervision"</i></a>, from Selsam et al. published in ICLR 2019.


The primary goal of the model is to predict whether a  SAT  formula is satisfiable, i.e. if there exists an instance of its variables such that the formula evaluates to `True`. The model is thus trained  to predict  <span class="keyword">the satisfiability</span> of a formula in conjunctive normal form. Interestingly, as a byproduct, an actual  satisfying assignment can be worked out from the network's activations in most cases when the formula is satisfiable.

   *  <span class="keyword">Pros (+):</span> No need for extensive annotation, seems to extrapolate nicely to harder problems by increasing the number  message passing iterations.
   *  <span class="keyword">Cons (-):</span> Limited practical applicability since it is outperformed by classical SAT solvers.
 
 
### <i class="fa fa-edit"></i>  NeuroSAT
 
#### Inputs
The inputs to the model are Boolean logic formulas in <span class="keyword">conjunctive normal form</span> (CNF), i.e., each input formula is represented as a conjunction ($$\land$$) of **clauses**, which are themselves disjunctions ($$\lor$$) of literals (positive or negative instances of variables). The goal is to learn a classifier to predict whether such a formula is satisfiable.
 
 A first issue that arises is to find a way to encode the input formula in such a way that it preserves CNF invariances, such as invariance to negating a literal in all clauses, invariance to permutations in $$\lor$$ and $$\land$$ etc. The authors use a standard   <span class="keyword">undirected graph representation</span> where:
   * $$\mathcal V$$: vertices are the literals (*positive and negative form of variables, denoted as $$x$$ and $$\bar x$$ respectively*) and the clauses occurring in the input formula.
   * $$\mathcal E$$: Edges are added to connect **(i)** the literals with clauses they appear in and **(ii)** each literal to its negative counterpart. 
   
The graph relations are encoded in an <span class="keyword">adjacency matrix</span>, $$A$$, with as many rows as there are literals and as many columns as there are clauses. Note that this structure does not constrain the vertices ordering, and does not make any preferential treatment between positive or negative literals.

The representation still has some caveats: For instance, when there are disconnected components in the graph, the averaging decision rule (*see next paragraph*) can lead to false positives. However, such structures can be detected and pre-processed away for convenience. 
   
   
#### Message-passing model
In a high-level view, the model keeps track of an embedding for each literal and each clause ($$L^t$$ and $$C^t$$), updated via <span class="keyword">message-passing</span> on the graph, and combined via a Multi Layer Perceptron (MLP) to output the  model prediction of the formula's satisfiability. Then the model updates are as follow:

$$
\begin{align}
C^t, h_C^t &= \texttt{LSTM}_\texttt{C}(h_C^{t - 1}, A^T \texttt{MLP}_{\texttt{L}}(L^{t - 1}) )\ \ \ \ \ \ \ \ \ \ \ (1)\\
L^t, h_L^t &= \texttt{LSTM}_\texttt{L}(h_L^{t - 1}, \overline{L^{t - 1}}, A\ \texttt{MLP}_{\texttt{C}}(C^{t }) )\ \ \ \ \ \ (2)\\
\end{align}
$$

where $$h$$ designates a hidden context vector for the LSTMs. The operator $$L \mapsto \bar{L}$$  returns $$\overline{L}$$, which is the  embedding matrix $$L$$ where the row of each literal is swapped with the one corresponding to the literal's negation.

In summary, in step **(1)** each clause embedding is updated based on the literal that composes it, while in step **(2)** each literal embedding is updated based on the clauses it appears in  and its negated counterpart.

After $$T$$ iterations of this message-passing scheme, the model outputs  logit for the satisfiability classification problem,

$$
\begin{align}
L_{\mbox{vote}} &= \texttt{MLP}_{\texttt{vote}}(L^T)\\
y &= \mbox{mean}(L_{\mbox{vote}})
\end{align}
$$

The pipeline is then trained end-to-end  via <span class="keyword">sigmoid cross-entropy</span>. The training set is built such that for any satisfiable training formula $$S$$, it also includes an unsatisfiable counterpart $$S'$$ which differs from $$S$$ <span class="keyword">only by negating one literal in one clause</span>. The hope is that these carefully curated samples constrain the model to pick up substantial characteristics of the formula.

In practice, the model is trained on formulas containing up to ***40 variables***, and on average ***200 clauses***. At this size, the SAT problem can still be solved by state-of-the-art solvers (yielding the supervision) but are large enough they prove challenging for Machine Learning models.


#### Inferring the SAT assignment
When a formula is satisfiable, one often also wants to find a <span class="keyword">valuation</span> (variable assignment) that satisfies it.
Recall that $$L_{\mbox{vote}}$$ encodes a "vote" for every literal, hence could be a good candidate to determine an assignment. Qualitative experiments show that those scores cannot be directly used for inferring the variable assignment, but that they do induce a nice <span class="keyword"> clustering of the variables</span>, once the message passing has converged. Hence the authors propose the following process to find a satisfying valuation:

  * <span class="keyword">(1)</span> Reshape $$L{\mbox{vote}}$$  to size $$(n, 2)$$ where $$n$$ is the number of literals; i.e., each row now contains the vote for a literal and its negated counterpart. 
  * <span class="keyword">(2)</span> Cluster the literals into two clusters with centers $$\Delta_1$$ and $$\Delta_2$$ using the following criterion:
  \begin{align}
  \|x_i - \Delta_1\|^2 + \|\overline{x_i} - \Delta_2\|^2 \leq \|x_i - \Delta_2\|^2 + \|\overline{x_i} - \Delta_1\|^2
  \end{align}
  * <span class="keyword">(3)</span> Try the two resulting assignments (i.e., set $$\Delta_1$$ to `True` and $$\Delta_2$$ to `False`, or vice-versa) and choose the one that yields satisfiability if any.
 
 
 In practice, this method retrieves a satisfying assignment for over 70% of the satisfiable test formulas.


### Experiments

In practice, the <span class="keyword">NeuroSAT</span> model is trained with embeddings of dimension **128** and **26** message passing iterations. The MLPs architecture are standard: 3 dense layers followed by ReLU activations. The final model obtains **85%** accuracy for predicting a formula's satisfiability on the test set.

More interestingly, it seems it can generalize to <span class="keyword">larger problems</span>, although it requires to increase the number of message passing iterations. However the classification performance significantly decreases (e.g. 25% for 200 variables) and the number of iterations ***linearly scales*** with the number of variables in the reported experiments: While this is not  the most viable solution, it shows the model does learn some general characteristics of Boolean formulas. that can be applied for larger problems.

Similarly, the model also generalizes well to other classes of  problems that  were <span class="keyword">reduced to SAT</span> (using SAT's NP-completeness), although they have different structure than the random formulas generated for training.

<div style='width:100%; overflow:hidden; text-align:center;'>
<div style='width:30%; display:inline-block; vertical-align:middle;'>
 <center><img src='/images/readingnotes/neurosat_generalization.png'></center>
</div>
<div style='width:68%; display:inline-block; vertical-align:middle;'>
 <center><img src='/images/readingnotes/neurosat_dynamics.png'></center>
</div>
</div>
<br><b>Figure 1:</b> <b>(left)</b> Success rate of a NeuroSAT model trained on 40 variables for test set involving formulas with up to 200 variables, as a function of the number of message-passing iterations. <b>(right)</b> The sequence of literal votes across message-passing iterations on a satisfiable formula. The vote matrix is reshaped such that each row contains the votes for a literal and its negated counterpart. For several iterations, most literals vote `unsat` with low confidence (<font color='blue'>light blue</font>).  After a few iterations, there is a phase transition and all literals vote sat with very high confidence (<font color="red">dark red</font>), until convergence.
<br>
<br>
 

<span class="keyword">To summarize</span>, the model takes advantage of the structure of Boolean formulas, and is able to predict whether an input formula is satisfiable or not with high accuracy. Moreover, even though trained only with this weak supervisory signal, it can work out a valid assignment most of the time. However it is still subpar compared to standard SAT solvers, which makes it hard to apply in practice.
<br>
<br>