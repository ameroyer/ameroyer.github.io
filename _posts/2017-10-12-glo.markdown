---
layout: post
title:  "generative latent optimization"
date:   2017-10-12 10:00:00 +0200
categories: python, tensorflow, generative, vae
thumb: /images/thumbs/glo.jpg
---

This is a Tensorflow implementation of the Generative Latent Optimization (GLO) model as introdced in <a href="https://arxiv.org/abs/1707.05776" target="_blank"><i>Optimizing the Latent Space of Generative Networks</i>, P. Bojanowski, A. Joulin, D. Lopez-Paz, A. Szlam, ICLR 2017</a>.

In the context of Variational Autoencoders (see <a href="/python,/tensorflow,/generative,/vae/2017/10/01/vae.html" target="_blank">this previous post</a>), the GLO model can be seen as a VAE without an encoder, where the latent code are free variables. 
This considerably simplifies the model at training time, where we simply optimize the latent codes and the variables of the decoder based on a reconstruction loss. On the other hand, inference is ill-posed, as we do not have an explicit prior to sample from such as in VAEs for instance.

In order to palliate this problem, the GLO paper proposes to constrain the space of the latent codes by projecting them to the unit ball. Images are then generated by sampling new codes either from a unit variance Gaussian, or from a diagonal covariance Gaussian fitted to the codes in the training set. Both methods seem to yield reasonable samples in practice.


<div style="width:20%; float:right; margin-bottom:25px">
<table border="1" cellpadding="6" align="right">
<tr>
<td><a style="color:#5E412F" href="/notebooks/2017_10_12_GLO/vae_glo.ipynb">Download .ipynb notebook</a></td>
</tr>
</table>
</div>
<div style="width:80%; text-align:center">
<img src='/notebooks/2017_10_12_GLO/sampled_unit_grid_960001.jpg' style="width:35%px">&nbsp;&nbsp;&nbsp;
<img src='/notebooks/2017_10_12_GLO/interpolated_grid_960001.jpg' style="width:35%px">
</div>


First, we define the input loading queue which reads images from a given list of filenames and feed them through an optional preprocessing function.

The `get_inputs_queue` function returns a queue whose elements are input dictionnary with keys:

  * `image`: A 4D Tensor of size (BATCH_SIZE, HEIGHT, WIDTH, N_CHANNELS) representing the input images.
  * `index`: A scalar Tensor containing the index of the image in the database


```python
def get_inputs_queue(filenames, 
                     preprocess_inputs=None,
                     batch_size=16,
                     num_threads=1,
                     extension='jpeg', 
                     channels=3,
                     capacity=800,
                     min_after_dequeue=100):
    """Returns a queue containing input images and indices from a files list.
    
    Args:
      filenames: List of image files to read the input from.
      preprocess_inputs: Preprocessing function taking a dictionary as input 
        and outputting a dictionnary with the same keys.
      batch_size: Batch size.
      num_threads: Number of readers for the batch queue.
      extension: Image format.
      channels: Number of image channels.
      capacity: Queue capacity.
      
    Returns: 
      A queue containing the input batches, where each input is a dictionnary
      of Tensors.
    """
    inputs = {}
    # Read index
    index_queue = tf.train.range_input_producer(
        len(filenames), capacity=capacity, shuffle=False)
    inputs['index'] = index_queue.dequeue()
    
    # Read Image
    filename_queue = tf.train.string_input_producer(
        filenames, capacity=capacity, shuffle=False)
    _, reader = tf.WholeFileReader().read(filename_queue)
    if extension in ['jpg', 'jpeg']:
        image = tf.image.decode_jpeg(reader, channels=channels, name='decoder')
    else:
        print('%s unimplemented' % extension)
    inputs['image'] = image
        
    # Preprocess the inputs
    if preprocess_inputs is not None:
        with tf.variable_scope('inputs_preprocess'):
            inputs = preprocess_inputs(inputs)
    
    # Batch the inputs 
    inputs = tf.train.shuffle_batch(inputs, batch_size, capacity, 
                                    min_after_dequeue,
                                    num_threads=num_threads, 
                                    name='inputs_batches_queue')
    return inputs   

def preprocess_inputs(inputs, tight_crop=False, size=256):
    """Preprocess input images to map them to [0, 1] and square-resize them.
    
    Args:
      inputs: A dictionnary of Tensors.
      tight_crop: If True, further crops the face input.
      size: The square size to resize image to.
      
    Returns:
      The preprocessed dictionnary of inputs with normalized images.
    """
    # Map to [-1, 1]
    with tf.control_dependencies([tf.assert_type(inputs['image'], tf.uint8)]):
        inputs['image'] = tf.image.convert_image_dtype(
            inputs['image'], tf.float32)
        inputs['image'] = (inputs['image'] - 0.5) * 2
        
    # Central crop to minimal side
    height = tf.shape(inputs['image'])[0]
    width = tf.shape(inputs['image'])[1]
    min_side = tf.minimum(height, width)
    min_side = 108 if tight_crop else tf.minimum(height, width)
    offset_height = (height - min_side) // 2
    offset_width = (width - min_side) // 2
    inputs['image'] = tf.image.crop_to_bounding_box(
        inputs['image'], offset_height, offset_width, min_side, min_side)
    
    # Resize
    if size is not None and size > 0:
        inputs['image'] = tf.image.resize_images(inputs['image'], (size, size))
    
    return inputs
```

#### Encoder

As we mentionned, there is no encoder in GLO. We directly learn the code z(x) as a free variable for each input training image x, without imposing a parametric form of the latent code distribution. Note this implies that the number of variables grows linearly with the number of data samples. Additionally, the code space is constrained by projecting each code to the unit ball before feeding them to the decoder. Note 
  
#### Decoder

  * *Inputs*: (batch**x**num_latent_dims)
  * 1 deconvolution upscale the input to (batch**x**4**x**4**x**c)
  * 5 deconvolutional blocks
    * transpose convolution, stride 2, kernel size 3
    * ReLU activation and Batch normalization
  * *Outputs*: (batch**x**128**x**128**x**3)


```python
def project(z, as_numpy=False):
    """Project the input vector to the unit ball."""
    if as_numpy:
        return z / np.sqrt(np.sum(z ** 2, axis=1))[:, np.newaxis]
    else:
        return z / tf.sqrt(tf.reduce_sum(z ** 2, axis=1, keep_dims=True))
    
def decoder(latent_z, 
            num_filters,
            kernel_size=5,
            activation_fn=tf.nn.relu, 
            normalizer_fn=slim.batch_norm,
            normalizer_decay=0.99,
            is_training=True, 
            reuse=False):
    """ Simple convolutional decoder with deconvolutions and ReLU.
    
    Args:
      latent_z: 2D Tensor representing the latent variable.
      num_filters: Number of filters for each convolutional block.
      activation_fn: Activation function. Defaults to elu.
      normalizer_fn: Normalization function. Defaults to Batch norm. Set to
          None for no normalization.
      normalizer_decay: Decay for the normalization.
      is_training: Whether the model is in training mode.
      reuse: Whether to reuse the variables or not.
      
    Returns:
      A 4D Tensor representing the output images
      
    """
    # Config
    weights_initializer = tf.contrib.layers.xavier_initializer()
    normalizer_params = {'is_training': is_training, 'decay': normalizer_decay, 'scale':True}
    
    # Network
    with tf.variable_scope('decoder', reuse=reuse):
        with slim.arg_scope([slim.conv2d_transpose], stride=2,
                            weights_initializer=weights_initializer,
                            activation_fn=activation_fn,
                            normalizer_fn=normalizer_fn,
                            normalizer_params=normalizer_params,
                            padding='SAME'):
            # Flattened input -> 4 x 4 patch
            shape = latent_z.get_shape().as_list()
            net = tf.reshape(latent_z, (shape[0], 1, 1, shape[1]))
            net = slim.conv2d_transpose(net, num_filters[0], [4, 4],
                                        stride=1,
                                        padding='VALID', 
                                        scope='deconv1')
            # Upscale via deconvolutions
            for i, num_filter in enumerate(num_filters[1:]):
                net = slim.conv2d_transpose(net, num_filter, [kernel_size, kernel_size],
                                            scope='deconv%d' % (i + 2))
            
            # Final deconvolution
            net = slim.conv2d_transpose(net, 3, [3, 3], 
                                        stride=2,
                                        activation_fn=tf.nn.tanh,
                                        normalizer_fn=None,
                                        scope='deconv_out')
            return net    
        
def get_glo_loss(images, reconstructions, weight=1.0):
    """Returns the GLO loss
    
    Args:
        images: Input images
        reconstructions: Generated reconstructions
    """
    return weight * tf.reduce_mean(tf.square(images - reconstructions))
```

We're now ready for training. We first define hyperparameters for the model.
During training, the latent codes are directly optimized and are stored as trainable `Tensorflow` variables that can be retrieved by slicing with the corresponding index. Due to the large size of the dataset ($\sim$ 200k samples in CelebA), the model requires a few epochs starting to learn correct codes.

```python
with tf.Graph().as_default():  
    ## Input images
    filenames = glob.glob(os.path.join(BASE_DIR, '*.%s' % EXTENSION))
    if NUM_SAMPLES > 0: filenames = filenames[:NUM_SAMPLES]
    
    ## latent codes
    codes = slim.variable('codes', dtype=tf.float32, 
                          shape=(len(filenames), NUM_DIMS),
                          trainable=True)  
    
    ## Training Network
    for i in range(NUM_GPUS):
        # inputs
        with tf.name_scope('inputs_%d' % (i + 1)):
            inputs = get_inputs_queue(filenames,
                                      lambda x: preprocess_inputs(x, tight_crop=TIGHT_CROP, size=SIZE), 
                                      batch_size=BATCH_SIZE,
                                      extension=EXTENSION)
            
        # outputs
        outputs = {}
        with tf.name_scope('model_%d' % i):
            with tf.device('/gpu:%d' % i):
                # retrieve code
                outputs['code'] = tf.gather(codes, inputs['index'])
                # project code
                outputs['code'] = project(outputs['code'])
                # decode
                outputs['reconstruction'] = decoder(
                    outputs['code'],
                    num_filters=NUM_DECODER_FILTERS,
                    reuse=i > 0)
    
        # loss 
        with tf.name_scope('loss_%d' % i):
            loss =  tf.losses.mean_pairwise_squared_error(
                inputs['image'], outputs['reconstruction'])
            tf.add_to_collection('total_loss', loss)
            
    # Optimization
    global_step = tf.contrib.framework.get_or_create_global_step()
    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)
    loss = tf.add_n(tf.get_collection('total_loss')) / NUM_GPUS
    tf.summary.scalar('total', loss, family='losses')
    train_op = optimizer.minimize(loss, global_step=global_step, 
                                  colocate_gradients_with_ops=True)
    
    # Update op for batch norm
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) 
    train_op = tf.group(train_op, *update_ops)
        
    # Multi:gpu Track the moving averages of all trainable variables.
    if NUM_GPUS > 1:
        variable_averages = tf.train.ExponentialMovingAverage(0.9995, global_step)
        variables_averages_op = variable_averages.apply(tf.trainable_variables())
        # Group all updates to into a single train op.
        train_op = tf.group(train_op, variables_averages_op)
    
    ## Inference Network
    with tf.name_scope('sampled_unit'):
        sampled_z = tf.random_normal((BATCH_SIZE, NUM_DIMS))
        sampled_z = project(sampled_z)
        outputs['sampled_unit'] = decoder(sampled_z, 
                                          num_filters=NUM_DECODER_FILTERS,
                                          is_training=False,
                                          reuse=True)    
    with tf.name_scope('sampled_fitted'):
        z_means, z_vars = tf.nn.moments(codes, axes=(0,))
        z_means = tf.expand_dims(z_means, 0)
        z_vars = tf.expand_dims(z_vars, 0)
        sampled_z = tf.random_normal((BATCH_SIZE, NUM_DIMS))
        sampled_z = z_means + sampled_z * tf.sqrt(z_vars) 
        sampled_z = project(sampled_z)
        outputs['sampled_fitted'] = decoder(sampled_z, 
                                            num_filters=NUM_DECODER_FILTERS,
                                            is_training=False,
                                            reuse=True)   
    with tf.name_scope('interpolated'):
        indx = np.random.randint(0, len(filenames), size=2)
        sampled_z_1 = tf.expand_dims(tf.gather(
            codes, tf.random_uniform((), maxval=len(filenames), dtype=tf.int32)), 1)
        sampled_z_2 = tf.expand_dims(tf.gather(
            codes, tf.random_uniform((), maxval=len(filenames), dtype=tf.int32)), 1)
        coeffs = np.linspace(0., 1., NUM_ROWS * NUM_ROWS)
        sampled_z = sampled_z_1 * coeffs + sampled_z_2 * (1 - coeffs)
        sampled_z = tf.transpose(sampled_z, (1, 0))
        sampled_z = project(sampled_z)
        outputs['interpolated'] = decoder(sampled_z, 
                                          num_filters=NUM_DECODER_FILTERS,
                                          is_training=False,
                                          reuse=True)    
    
    ## Additional Summaries
    add_glo_summaries(inputs, outputs, num_rows=NUM_ROWS)
    add_glo_config_summary()
        
    ## Launch the training session
    try:
        global_step_ = 0
        LOG_DIR = os.path.join(BASE_LOG_DIR, EXP_NAME, datetime.now().strftime("%m-%d_%H-%M"))
        with get_monitored_training_session() as sess:
            while not sess.should_stop():
                # If save images
                if (global_step_ + 1) % SAVE_IMAGES_STEPS == 0:
                    out_ = sess.run([global_step, loss, train_op] + 
                                    [outputs[k] for k in SAVE_IMAGES_KEYS])
                    global_step_, loss_ = out_[0], out_[1]
                    for k, img in zip(SAVE_IMAGES_KEYS, out_[3:]):
                        scipy.misc.imsave(
                            os.path.join(LOG_DIR, 'images', 
                                         '%s_%d.jpg' % (k, global_step_ + 1)), 
                            img[0])   
                # Normal step
                else:
                    global_step_,loss_, _ = sess.run([global_step, loss, train_op])
                print('\rStep %d: %.3f' % (global_step_, loss_), end='')
    except KeyboardInterrupt:
        print('\nInterrupted at step %d' % global_step_)
```

<div style="text-align: center">
<img src="/notebooks/2017_10_12_GLO/output_12_1.png">
</div>

We also observe the model does not seem to overfit to the training dataset and is able to generate unseen samples. See for instance the result of nearest neighbor search across the training dataset:


<div style="text-align: center">
<img src="/notebooks/2017_10_12_GLO/output_14_1.png">
</div>

