---
layout: post
title:  "variational auto-encoders"
date:   2017-10-01 10:00:00 +0200
categories: python, tensorflow, generative, vae
thumb: /images/thumbs/vae.jpg
---

This post presents a simple Tensorflow implementation of the Variational Auto-encoder model (VAE) introduced in <a href="https://arxiv.org/abs/1312.6114" target="_blank"><i>Auto-Encoding Variational Bayes</i>, D. Kingma, M. Welling, ICLR 2017</a>, that I will apply to the <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank">CelebA</a> faces dataset here.

     
### <i class="fa fa-edit"></i> Model

Variational Auto-encoders (VAE) are <span class="keyword">probabilistic generative models</span> relying on a simple latent representation that captures the input data intrinsic properties. Given latent code $$z$$ sampled from a prior distribution <span>$$p_{\theta}(z)$$</span>, we generate a sample $$x$$ from the conditional <span>$$p_{\theta}(x\ |\ z)$$</span>. The goal is to learn the parameters of this generative model as well as how to map data points to latent codes.

In many cases however, the posterior distribution <span>$$p_{\theta}(z\ |\ x)$$</span> is intractable. It is instead approximated by a parametric model <span>$$q_{\phi}(z\ |\ x)$$</span>, where $$\phi$$ are called the variational parameters (see the graphical model below). In the following, we will drop the $$\theta$$ and $$\phi$$ notations for simplicity. 



<div style="width:20%; float:right">
<table border="1" cellpadding="6" align="right">
<tr>
<td><a style="color:#5E412F" href="/notebooks/2017_10_1_VAE/vae.ipynb">Download .ipynb notebook</a></td>
</tr>
</table>
</div>
<div style="width:80%; text-align:center; margin-bottom:25px">
<img src='/notebooks/2017_10_1_VAE/autoencoder.png' style="width:25%">
<img src='/notebooks/2017_10_1_VAE/sampled_grid_60001.jpg' style="width:35%">
<img src='/notebooks/2017_10_1_VAE/interpolated_grid_660001.jpg' style="width:35%">
</div>

To summarize, a VAE is composed from an <span class="keyword">encoder</span> <span>$$q(z\ |\ x)$$</span>, which maps an input $$x$$  to a latent representation $$z$$, typically of much lower dimension, and a <span class="keyword">decoder</span> <span>$$p(x\ |\ z)$$</span> that generates sample $$x$$ from a latent code $$z$$. Both of these mappings are parametrized as neural networks in practice, and our goal is to find their optimal parameters by maximizing the data likelihood, $$p(x)$$. 


#### Training objective 

As the likelihood is intractable, we indeed derive the following <span class="keyword">variational lower bound</span> (also known as $${\mathcal L}_{ELBO}$$) on the  data log-likelihood:

<div style="text-align: center; margin:10px">
$$
\begin{align}
\log p(x) &= \mbox{KL}(q(z\ |\ x)\ \|\ p(z\ |\ x)) - {\mathcal L}_{ELBO}\geq - {\mathcal L}_{ELBO}\\
{\mathcal L}_{ELBO} &= \mbox{KL}(q(z\ |\ x)\ \|\ p(z)) - \mathbb{E}_{z \sim q(z\ |\ x)} (\log p(x\ |\ z))
\end{align}
$$
</div>

The model is trained by minimizing  $${\mathcal L}_{ELBO}$$; The right term is typically interpreted as a reconstruction loss term, given codes sampled from the encoder distribution, while the left term acts as a regularizer and  is the KL divergence between the approximate posterior and the prior $$p(z)$$.
Finally, note that this bound is optimal when the encoder perfectly approximates the true posterior, i.e., <span>$$\mbox{KL}(q(z\ |\ x)\ \|\ p(z\ |\ x)) = 0$$</span>.


#### Parametrization

The vanilla VAE is parametrized with <span class="keyword">Gaussian</span> function as follows:

  * The latent code prior is <span>$$p(z) = \mathcal{N}(z\ |\ 0, 1)$$</span>
  *  <span>$$q(z\ |\ x) = \mathcal{N}(z\ |\ \mu_q(x), \sigma_q(x))$$</span> is a Gaussian with diagonal covariance, where $$\mu_q$$ and $$\sigma_q$$ are output by the encoder network.
  * <span>$$p(x\ |\ z) = \mathcal{N}(x\ |\ \mu_p(z), \sigma_p)$$</span> is a Gaussian with diagonal covariance, where $$\mu_p$$ is the reconstruction output by the decoder network and $$\sigma_p \in \mathbb{R}$$ is an hyperparameter.

### <i class="fa fa-wrench"></i> Implementation

#### Inputs pipeline
First, we define the input loading queue which reads images from a given list of filenames and feeds them through an optional preprocessing function. I use Tensorflow queues utilities rather than placeholder so all input-related operations are built in the static graph directly.

The <span class="inline-code">get_inputs_queue</span> function returns a queue whose elements are input dictionary with key "image": A 4D Tensor of size  <span class="inline-code">(batch_size, height, width, num_channels)</span> representing the inputs.



```python
# Read Image from file
inputs = {}    
filename_queue = tf.train.string_input_producer(
    filenames, capacity=capacity, shuffle=False)
_, reader = tf.WholeFileReader().read(filename_queue)
image = tf.image.decode_jpeg(reader, channels=channels, name='decoder')
inputs['image'] = image
        
# Preprocess the inputs
with tf.variable_scope('inputs_preprocess'):
    inputs = preprocess_inputs(inputs)
    
# Batch and shuffle the inputs 
inputs = tf.train.shuffle_batch(
    inputs,  batch_size, capacity, min_after_dequeue)
```

The  <span class="inline-code">preprocess_inputs</span> function simply performs a central crop on the input image, resizes them to square size 128x128 and finally maps them to [-1, 1].

```python
# Map to [-1, 1]
with tf.control_dependencies([tf.assert_type(inputs['image'], tf.uint8)]):
   inputs['image'] = tf.image.convert_image_dtype(
       inputs['image'], tf.float32)
   inputs['image'] = (inputs['image'] - 0.5) * 2
        
# Central crop to minimal side
height = tf.shape(inputs['image'])[0]
width = tf.shape(inputs['image'])[1]
min_side =  tf.minimum(height, width)
offset_height = (height - min_side) // 2
offset_width = (width - min_side) // 2
inputs['image'] = tf.image.crop_to_bounding_box(
    inputs['image'], offset_height, offset_width, min_side, min_side)
    
# Resize
if size is not None and size > 0:
    inputs['image'] = tf.image.resize_images(inputs['image'], (size, size))
```

#### Architecture
For the feed-forward network, I use a rather simple convolutional architecture with ReLU activations, batch normalization layers and max-pooling.
More specifically, the encoder is described and implemented as follow

**_Encoder_**

* *Inputs*:  <span class="inline-code">(batch size, 128, 128, 3)</span> in [-1, 1]
* 5 convolutional blocks
* Convolutions, stride 2
* ReLU activation and Batch normalization
* Max-pooling
* Final block:  <span class="inline-code">(batch size, 4, 4, c)</span>
* 2 separate fully-connected layers
* *Outputs*:  $$\mu_q$$ and $$\log (\sigma_q)$$, each  of size <span class="inline-code">(batch size, num_latent_dims)</span>

```python
with tf.variable_scope('encoder', reuse=reuse):
   # Convolutions
    with slim.arg_scope([slim.conv2d],
			stride=2,
			weights_initializer=weights_initializer,
			activation_fn=activation_fn,
			normalizer_fn=normalizer_fn,
			normalizer_params=normalizer_params,
			padding='SAME'):
        net = inputs
        for i, num_filter in enumerate(num_filters):
            net = slim.conv2d(net, num_filter,
	        [kernel_size, kernel_size], scope='conv%d' % (i + 1))
                
    # Fully connected
    net = tf.contrib.layers.flatten(net)
        
    with slim.arg_scope([slim.fully_connected],
			weights_initializer=weights_initializer,
			normalizer_fn=None,
			activation_fn=None):
        z_mean = slim.fully_connected(net, num_dims)
        z_log_var = slim.fully_connected(net, num_dims)
```

**_Decoder_**

* *Inputs*:  <span class="inline-code">(batch size, num_latent_dims)</span>
* 1 deconvolution upscale the input to  <span class="inline-code">(batch size, 4,  4, c)
* 5 deconvolutional blocks
* transpose convolution, stride 2
* ReLU activation and Batch normalization
* *Outputs*: $$\mu_p$$,  <span class="inline-code">(batch size, 128, 128, 3)</span> in [-1, 1], mean of the image distribution

```python
with tf.variable_scope('decoder', reuse=reuse):
    with slim.arg_scope([slim.conv2d_transpose],
			stride=2,
			weights_initializer=weights_initializer,
			activation_fn=activation_fn,
			normalizer_fn=normalizer_fn,
			normalizer_params=normalizer_params,
			padding='SAME'):
        # Flattened input -> 4 x 4 patch
        shape = latent_z.get_shape().as_list()
        net = tf.reshape(latent_z, (shape[0], 1, 1, shape[1]))
        net = slim.conv2d_transpose(net, num_filters[0], [4, 4], stride=1,
	    padding='VALID', scope='deconv1')

        # Upscale via deconvolutions
        for i, num_filter in enumerate(num_filters[1:]):
            net = slim.conv2d_transpose(net, num_filter, 
	        [kernel_size, kernel_size], scope='deconv%d' % (i + 2))
            
        # Final deconvolution
        net = slim.conv2d_transpose(net, 3, [3, 3], stride=2,
	    activation_fn=tf.nn.tanh, normalizer_fn=None, scope='deconv_out')
```

#### Loss function
Now that we have the main architecture, we need to define the training loss function. In our particular setting, the $${\mathcal L}_{ELBO}$$ can be simplified as follows.

First, $$\sigma_p$$ is taken as a constant in $$\mathbb{R}$$. This means we can simplify the reconstruction loss term in $${\mathcal L}_{ELBO}$$:
	
<div style="text-align: center; margin:10px">
$$
\begin{align}
 \mathbb{E}_{z \sim q(z\ |\ x)} (\log p(x\ |\ z)) &= -\frac{1}{2} \mathbb{E}_{z \sim q(z\ |\ x)}  \sum_{i=1}^{D} \log (2 \pi ) + \log \sigma_p + \frac{(x_i - \mu_p(z)_i)^2}{\sigma_p}\\
						  &= C - \frac{1}{2\sigma_p}  \mathbb{E}_{z \sim q(z\ |\ x)} \| x - \mu_p(z) \|^2
\end{align}
$$
</div>

where $$C$$ is a constant we can safely ignore for the loss and $$D$$ is the dimensionality of $$x$$. The above equation shows that $$\sigma_p$$ acts as a weighting factor on the reconstruction loss term between $$x$$ and the output decoder mean $$\mu_p(z)$$. In particular, when $$\sigma_p \rightarrow 0$$, we revert to a classical auto-encoder where the reconstruction loss term totally overweights the latent loss term in  $${\mathcal L}_{ELBO}$$.


Secondly, since the prior distribution and the encoder distribution are Gaussian with diagonal covariance, the KL divergence term can be expressed in analytical form (<a href="https://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians/60699" target="_blank">see for instance</a>):

<div style="text-align: center; margin:10px">
$$
\begin{align}
\mbox{KL}(q(z\ |\ x)\ \|\ p(z)) &= \mbox{KL}\left(\mathcal{N}(\mu_q, \sigma_q)\ |\ \mathcal{N}(0, 1) \right)\\
 &= \frac{1}{2} \sum_{i=1}^d - \log (\sigma_q^i) - 1  + \sigma_q^i +  {\mu_q^i}^2 
\end{align}
$$
</div>

where $$d$$ is the dimension of the latent space.


In practice, we approximate expectations with sum over samples, and we use vector arithmetics, which leads to the following expression for each term:

  * The  <span class="inline-code">pixel loss</span> is the expectation of the decoder output under the latent codes distribution generated by the encoder
  <div style="text-align:center">
$$
  \begin{align}
  \mathcal{L}_{pixel}(X, \hat{X}) = \frac{1}{w \times h} \sum_{i=1}^w \sum_{j=1}^h \left( X_{i,j} - \hat{X}_{i, j} \right)^2
  \end{align}
$$
  </div>
  * The  <span class="inline-code">latent loss</span> is the KL-divergence between the encoder distribution <span>$$q(z\ |\ x)$$</span> (Gaussian with diagonal covariance matrix) and the prior <span>$$p(z) = \mathcal{N}(z\ |\ 0, 1)$$</span>
  <div style="text-align:center">
$$
  \begin{align}  
  \mathcal{L}_{latent}(\mu, \sigma) = 0.5 \left( \mu^2 + \sigma - \log(\sigma) - 1 \right)
  \end{align}
$$
</div>


```python
def get_pixel_loss(images, reconstructions, weight=1.0):
    return weight * tf.reduce_mean(tf.square(images - reconstructions))

def get_latent_loss(z_mean, z_log_var, weight=1.0):
    return weight * 0.5 * tf.reduce_mean(tf.reduce_sum(
        tf.square(z_mean) + tf.exp(z_log_var) - z_log_var - 1., axis=1))
```


#### Training

Now we're ready for training. We will use the default Adam optimizer.
In the original notebook, I additionally define a few utilities function for Tensorboard summaries. The main VAE summaries contain the image reconstructions, sample generations and scalar summary for losses. 
I also use a  <span class="inline-code">MonitoredTrainingSession</span> that will take care of starting the input queues, defining the summary writer etc.

The following hyperparameters can be defined in the code:

*  <span class="inline-code">NUM_GPUS</span>, number of GPUs to use in experiments
*  <span class="inline-code">GPU_MEM_FRAC</span>, fraction of RAM to allocate per GPU
*  <span class="inline-code">BATCH_SIZE</span>, batch size
*  <span class="inline-code">SIZE</span>, input image size
*  <span class="inline-code">NUM_DIMS</span>, number of dimensions of the latent code
*  <span class="inline-code">NUM_ENCODER_FILTERS</span>, list of filter numbers for each convolutional block in the encoder
*  <span class="inline-code">NUM_DECODER_FILTERS</span>, list of filter numbers for each convolutional block in the decoder
*  <span class="inline-code">LEARNING_RATE</span>, base learning rate
*  <span class="inline-code">LATENT_LOSS_WEIGHT</span>, weight for the latent loss term (directly related to $$\sigma_p$$)
*  <span class="inline-code">ADAM_MOMENTUM</span>, $$\beta_1$$ parameter in the ADAM optimizer


```python
with tf.Graph().as_default():    
    ## Training Network
    for i in range(NUM_GPUS):
        # inputs
        with tf.name_scope('inputs_%d' % (i + 1)):
            inputs = get_inputs_queue(BASE_DIR, batch_size=BATCH_SIZE)
            
        # outputs
        outputs = {}
        with tf.name_scope('model_%d' % i):
            with tf.device('/gpu:%d' % i):
                # encode
                outputs['z_means'], outputs['z_log_vars'] = encoder(
                    inputs['image'], 
                    NUM_ENCODER_FILTERS,
                    NUM_DIMS, 
                    reuse=i > 0)
                # sample
                z_eps = tf.random_normal((BATCH_SIZE, NUM_DIMS))
                outputs['latent_z'] = (outputs['z_means'] + z_eps * 
                                       tf.exp(outputs['z_log_vars'] / 2))
                # decode
                outputs['reconstruction'] = decoder(
                    outputs['latent_z'],
                    NUM_DECODER_FILTERS, 
                    reuse=i > 0)
    
        # loss 
        with tf.name_scope('loss_%d' % i):
            pixel_loss =  get_pixel_loss(
	        inputs['image'], outputs['reconstruction'])
            latent_loss = get_latent_loss(outputs['z_means'],
					  outputs['z_log_vars'], 
                                          weight=LATENT_LOSS_WEIGHT)
            
    # Optimization
    global_step = tf.contrib.framework.get_or_create_global_step()
    optimizer = tf.train.AdamOptimizer(
        learning_rate=LEARNING_RATE, beta1=ADAM_MOMENTUM)
    pixel_loss = tf.add_n(tf.get_collection('total_pixel_loss')) / NUM_GPUS
    latent_loss = tf.add_n(tf.get_collection('total_latent_loss')) / NUM_GPUS
    loss = pixel_loss + latent_loss
    train_op = optimizer.minimize(loss, global_step=global_step, 
                                  colocate_gradients_with_ops=True)
    
    # Add update operations for Batch norm
    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) 
    train_op = tf.group(train_op, *update_ops)
            
    ## Launch the training session
    try:
        with get_monitored_training_session() as sess:
            while not sess.should_stop():
                    global_step_,loss_, _ = sess.run([global_step, loss, train_op])
                print('\rStep %d: %.3f' % (global_step_, loss_), end='')
    except KeyboardInterrupt:
        print('\nInterrupted at step %d' % global_step_)                    
```

#### Results
After training the model for some time on the <span class="keyword">CelebA</span> dataset, I obtain the following results (reconstructions, samples and interpolation).
![png](/notebooks/2017_10_1_VAE/output_13_1.png)

As is typical for VAEs under a Gaussian assumption, the resulting samples are rather blurry.
However the model is able to generate new unseen samples.
In fact, we can search for the nearest neighbors of the generated images in the training set (in terms of L2 distance), to check for any potential overfitting problems (which does not seem to be the case here):


<div style="text-align:center">
<img src="/notebooks/2017_10_1_VAE/output_15_1.png">
</div>
